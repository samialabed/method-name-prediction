# Method Name Prediction
This repository contains the Keras Implementation of [A convolutional attention network for extreme summarization of source code](https://arxiv.org/abs/1602.03001) [1]

The model takes in a sequence of subtokens composed of Java's method body and output an extreme summarisation in form of predicted method name.

Status:
* Successfully reproduce (and improve) results of the Convolution Attention Model.
* The Copy Attention Model is struggling to learn useful features - the code exist and compliment notebooks to allow further investigation.

## Setup
### Prerequisites 
The easiest way to install the prerequisites is to use [Anaconda](https://conda.io/en/latest/). 

```bash
# Install the environment
conda env create --file=environment.yml

# Activate the environment
source activate method-name-prediction

```

The dependencies contains jupyter dependency and can be started using `jupyter notebook`
and there are examples in the [notebooks directory](https://github.com/samialabed/method-name-prediction/tree/master/notebooks).


### Dataset

The model can be generalised to any dataset.

However, the preprocessors and utility functions are written to work with specific type of data available to students enrolled in [R252 - Machine learning in Programming](https://www.cl.cam.ac.uk/teaching/1819/R252/) at Cambridge University.

The expected input data format is .proto files that contains a feature graph of Java programs.
The feature graph can be generated by compiling Java programs with the features extractor extension enabled from `https://github.com/acr31/features-javac` 

## Usage Instructions

To execute training -> evaluation -> inference and output the results to a file: 

```python src/run_model.py [options] DATA_DIR PATH_TO_CONFIG_FILE ```

Where: 
* `DATA_DIR`: The path to the input data (training/testing/validating) - the preprocessor will split the input to 65% training, 5% validating, and 30% to test and inference.
* `PATH_TO_CONFIG_FILE`: The model hyperparameters as json config file. Expected of config files are in the [configs directory](https://github.com/samialabed/method-name-prediction/tree/master/configs)
* `[options]` can be any of the following:
  * `--trained-model-dir=DIR` where DIR is the path towards a previously trained model.
  * `--use-same-input-dir=bool` if specified the evaluation/inference will use the exact same data used to train and validate the model intended to reproduce the same results. 
  * `--help` to show help screen.


The model will create a directory in the trainied_models

```Output directory: trained_models/<NAME OF THE MODEL>/<RUN_NAME>/<DATE AND TIME>/*```


Output files:
* `config.json` Copy of the hyperparameters used in training the model.
* `inputs.txt` Stats about the input including how many methods used in training/testing/validating.
* `results.txt` The model f1 score, unknown accuracy, and exact copy accuracy.
* `model_accuracy.png` and `model_loss.png` The training/validating model loss and accuracy.
* `visualised_results.txt` randomly selected 10 predictions and visualised.
* Various hdf5 and pkl files meant to aid the reproducibility of your evaluation.


Full beamsearch is used in the inference - therefore the model inference takes a long time to compute. Using smaller size beam width can help the performance.

### Reproducing Evaluation Results
#### Model trained on Elasticsearch corpus excluding the unit tests
To reproduce the results of the model trained on Elasticsearch corpus excluding unittests:
```bash
python src/run_model.py \
    --trained-model-dir=trained_models/cnn_attention/elasticsearch_with_no_tests/2019-03-09-16-12/ \
    --use-same-input-dir \
    'data/raw/r252-corpus-features/org/elasticsearch/' \
    'configs/es-no-tests-cnn-attention.json'
```
The model will generate F1-results and predictions and output them to files in the same training directory.

#### Model trained on Elasticsearch corpus excluding the unit tests
To reproduce the results of the model trained on Elasticsearch corpus excluding unittests:
```bash
python src/run_model.py \
    --trained-model-dir=trained_models/cnn_attention/elasticsearch_with_tests/2019-03-09-23-45/ \
    --use-same-input-dir \
    'data/raw/r252-corpus-features/org/elasticsearch/' \
    'configs/es-no-tests-cnn-attention.json'
```
The model will generate F1-results and predictions and output them to files in the same training directory.


### Directory structure

* [configs](https://github.com/samialabed/method-name-prediction/tree/master/configs): Contains hyperparameters used in training the model and running the preprocessor. src/run_model.py validates the needed parameters.
* [data](https://github.com/samialabed/method-name-prediction/tree/master/data): (Git Ignored directory) Contains the raw .proto files used to train/test the model.
* [notebooks](https://github.com/samialabed/method-name-prediction/tree/master/notebooks): Contains example notebooks used to execute the model and archived notebooks used in training/testing.
* [src](https://github.com/samialabed/method-name-prediction/tree/master/src): The directory contains the code for the model. 
* [trained_models](https://github.com/samialabed/method-name-prediction/tree/master/trained_models/): the output directory for any experiment.

## References 
````
[1] Allamanis, M., Peng, H. and Sutton, C., 2016, June. 
A convolutional attention network for extreme summarization of source code.  
In International Conference on Machine Learning (pp. 2091-2100).

@inproceedings{allamanis2016convolutional,
  title={A Convolutional Attention Network for Extreme Summarization of Source Code},
  author={Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2016}
}
````
