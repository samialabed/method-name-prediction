{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.preprocess import PreProcessor\n",
    "\n",
    "data = PreProcessor(config=PreProcessor.DEFAULT_CONFIG,\n",
    "                    data_dir='data/raw/r252-corpus-features/org/elasticsearch/action/admin/cluster/allocation/')\n",
    "\n",
    "vocab = data.metadata['token_vocab']\n",
    "processed = data.get_tensorise_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 319 Code snippet len: 135 label_name len: 135\n(135, 50, 319)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "import numpy as np\n",
    "from models.cnn_attention import ConvAttention\n",
    "\n",
    "embedding_dim = 128\n",
    "vocabulary_size = len(vocab)\n",
    "max_chunk_length = data.config['max_chunk_length']\n",
    "code_snippet = processed['body_tokens']\n",
    "label_name = keras.utils.to_categorical(processed['name_tokens'], num_classes=vocabulary_size)\n",
    "print(\"Vocab Size: {} Code snippet len: {} label_name len: {}\".format(vocabulary_size, len(code_snippet), len(label_name)))\n",
    "\n",
    "# TODO make the input a json file and parse it\n",
    "batch_size = 1\n",
    "k1 = 8\n",
    "k2 = 8\n",
    "w1 = 24\n",
    "w2 = 29\n",
    "w3 = 10\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Optimised hyperparameter are reported in page 5 of the paper\n",
    "\n",
    "print(label_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Tensor(\"embedding_140/embedding_lookup/Identity_2:0\", shape=(1, 50, 128), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 6s - loss: 5.2925 - acc: 0.8268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 4.3085 - acc: 0.9593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 3.3875 - acc: 0.9593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 2.6482 - acc: 0.9593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 2.2154 - acc: 0.9593\n"
     ]
    }
   ],
   "source": [
    "# define layers\n",
    "import tensorflow as tf\n",
    "\n",
    "main_input = layers.Input(shape=(max_chunk_length,),\n",
    "                          batch_size=batch_size,\n",
    "                          dtype='int32', name='main_input')\n",
    "\n",
    "embedding_layer = layers.Embedding(vocabulary_size,\n",
    "                                   embedding_dim,\n",
    "                                   input_length=max_chunk_length)\n",
    "bias_vector = layers.Embedding(vocabulary_size, 1)\n",
    "\n",
    "gru_layer = layers.GRU(k2, return_state=True, stateful=True)\n",
    "\n",
    "# attention feature\n",
    "conv1 = layers.Conv1D(k1, w1, activation='relu', padding='causal')\n",
    "conv2 = layers.Conv1D(k2, w2, padding='causal')\n",
    "multiply_layer = layers.Multiply()\n",
    "dropout = layers.Dropout(dropout_rate)\n",
    "l2_norm = layers.Lambda(lambda x: keras.backend.l2_normalize(x, axis=1))\n",
    "\n",
    "# attention weight\n",
    "conv3 = layers.Conv1D(1, w3, activation='softmax', padding='causal', use_bias=True)\n",
    "\n",
    "#outputs\n",
    "masking_layer = layers.Masking(mask_value=0)\n",
    "softmax_layer = layers.Softmax()\n",
    "layers.Add\n",
    "reduce_sum_layer = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))\n",
    "transpose_layer = layers.Lambda(lambda x: keras.backend.transpose(x))\n",
    "addition_layer = layers.Lambda(lambda xy: xy[0] + xy[1])\n",
    "expand_dim = layers.Lambda(lambda x: keras.backend.expand_dims(x, axis=2))\n",
    "broadcast_multi = layers.Lambda(lambda xy: xy[0] * xy[1])\n",
    "# execution\n",
    "\n",
    "tokens_embedding = embedding_layer(main_input)\n",
    "print(\"tokens: {}\".format(tokens_embedding))\n",
    "bias = bias_vector(main_input)\n",
    "_, h_t = gru_layer(tokens_embedding)\n",
    "#l_feat\n",
    "L_1 = conv1(tokens_embedding)\n",
    "L_1 = dropout(L_1)\n",
    "L_2 = multiply_layer([L_1, h_t])\n",
    "L_2 = dropout(L_2)\n",
    "L_feat = l2_norm(L_2)\n",
    "#weights\n",
    "attention_weight = conv3(L_feat)\n",
    "alpha = dropout(attention_weight)\n",
    "n = layers.Dense(vocabulary_size, activation='softmax')(alpha)\n",
    "\n",
    "# alpha_emb = multiply_layer([alpha, tokens_embedding])\n",
    "# n_hat = reduce_sum_layer(alpha_emb)\n",
    "# n_t = expand_dim(transpose_layer(n_hat))\n",
    "# E = broadcast_multi([tokens_embedding, n_t])\n",
    "# print(E.shape)\n",
    "# n = softmax_layer(addition_layer([E, bias]))\n",
    "optimizer = keras.optimizers.Nadam()  # RMSprop with Nesterov momentum\n",
    "loss_func = keras.losses.categorical_crossentropy\n",
    "# define execution\n",
    "model = keras.Model(inputs=[main_input], outputs=n)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_func,\n",
    "              metrics=['accuracy'])\n",
    "# fit the model\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = model.fit(code_snippet, label_name, epochs=5, verbose=2, batch_size=batch_size, callbacks=[tbCallBack])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.21367346, 0.00559455, 0.00198038, ..., 0.00207148,\n         0.01201825, 0.01178893],\n        [0.21367346, 0.00559455, 0.00198038, ..., 0.00207148,\n         0.01201825, 0.01178893],\n        [0.21367346, 0.00559455, 0.00198038, ..., 0.00207148,\n         0.01201825, 0.01178893],\n        ...,\n        [0.21367346, 0.00559455, 0.00198038, ..., 0.00207148,\n         0.01201825, 0.01178893],\n        [0.21367346, 0.00559455, 0.00198038, ..., 0.00207148,\n         0.01201825, 0.01178893],\n        [0.21367346, 0.00559455, 0.00198038, ..., 0.00207148,\n         0.01201825, 0.01178893]]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(code_snippet[0].reshape(1, -1), steps=1)\n",
    "\n",
    "# model.predict(code_snippet[0].reshape(1, -1), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nmain_input (InputLayer)         (1, 50)              0                                            \n__________________________________________________________________________________________________\nembedding_140 (Embedding)       (1, 50, 128)         40832       main_input[0][0]                 \n__________________________________________________________________________________________________\nconv1d_210 (Conv1D)             (1, 50, 8)           24584       embedding_140[0][0]              \n__________________________________________________________________________________________________\ndropout_70 (Dropout)            multiple             0           conv1d_210[0][0]                 \n                                                                 multiply_64[0][0]                \n                                                                 conv1d_212[0][0]                 \n__________________________________________________________________________________________________\ngru_70 (GRU)                    [(1, 8), (1, 8)]     3288        embedding_140[0][0]              \n__________________________________________________________________________________________________\nmultiply_64 (Multiply)          (1, 50, 8)           0           dropout_70[0][0]                 \n                                                                 gru_70[0][1]                     \n__________________________________________________________________________________________________\nlambda_250 (Lambda)             (1, 50, 8)           0           dropout_70[1][0]                 \n__________________________________________________________________________________________________\nconv1d_212 (Conv1D)             (1, 50, 1)           81          lambda_250[0][0]                 \n__________________________________________________________________________________________________\ndense_21 (Dense)                (1, 50, 319)         638         dropout_70[2][0]                 \n==================================================================================================\nTotal params: 69,423\nTrainable params: 69,423\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.92592702971564\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# # overfit and evaluate the model \n",
    "loss, accuracy = model.evaluate(code_snippet, label_name, batch_size=1, verbose=0)\n",
    "print('Accuracy: {}'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 319)\n"
     ]
    }
   ],
   "source": [
    "# translate prediction\n",
    "\n",
    "from data.utils import translate_tokenized_array_to_list_words\n",
    "\n",
    "prediction = model.predict(code_snippet[5].reshape(1, -1))\n",
    "# translate_tokenized_array_to_list_words(vocab, processed['body_tokens'][1])\n",
    "print(prediction)\n",
    "# vocab.get_name_for_id(317)\n",
    "# processed['body_tokens'][0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
