{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# %load_ext watermark\n",
    "# %watermark -v -n -m -p numpy,scipy,sklearn,pandas,tensorflow,keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # files: 638\nTraining Data: 401, Testing Data: 192, Validating data: 45\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.estimators._sklearn import train_test_split\n",
    "\n",
    "from data.preprocess import PreProcessor, get_data_files_from_directory\n",
    "\n",
    "# Move this to a config file\n",
    "skip_tests = True  # skip files that contain test\n",
    "all_files = get_data_files_from_directory(data_dir='data/raw/r252-corpus-features/org/elasticsearch/',\n",
    "                                          skip_tests=skip_tests)\n",
    "print(\"Total # files: {}\".format(len(all_files)))\n",
    "train_data_files, test_data_files = train_test_split(all_files, train_size=0.7)\n",
    "train_data_files, validate_data_files = train_test_split(train_data_files, train_size=0.9)\n",
    "print(\"Training Data: {}, Testing Data: {}, Validating data: {}\".format(len(train_data_files),\n",
    "                                                                        len(test_data_files),\n",
    "                                                                        len(validate_data_files)))\n",
    "training_dataset_preprocessor = PreProcessor(config=PreProcessor.DEFAULT_CONFIG,\n",
    "                                             data_files=train_data_files)\n",
    "validating_dataset_preprocessor = PreProcessor(config=PreProcessor.DEFAULT_CONFIG,\n",
    "                                               data_files=validate_data_files,\n",
    "                                               metadata=training_dataset_preprocessor.metadata)\n",
    "testing_dataset_preprocessor = PreProcessor(config=PreProcessor.DEFAULT_CONFIG,\n",
    "                                            data_files=test_data_files,\n",
    "                                            metadata=training_dataset_preprocessor.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = training_dataset_preprocessor.metadata['token_vocab']\n",
    "vocabulary_size = len(vocab) + 1\n",
    "max_chunk_length = training_dataset_preprocessor.config['max_chunk_length']\n",
    "hyperparameter = {'batch_size': 1, 'k1': 8, 'k2': 8, 'w1': 24, 'w2': 29, 'w3': 10, 'dropout_rate': 0,\n",
    "                  'max_chunk_length': max_chunk_length, 'vocabulary_size': vocabulary_size, 'embedding_dim': 128}\n",
    "\n",
    "training_data_tensors = training_dataset_preprocessor.get_tensorise_data()\n",
    "testing_data_tensors = testing_dataset_preprocessor.get_tensorise_data()\n",
    "validating_data_tensors = validating_dataset_preprocessor.get_tensorise_data()\n",
    "\n",
    "# code_snippet = processed['body_tokens']\n",
    "training_body_subtokens = np.expand_dims(training_data_tensors['body_tokens'], axis=-1)\n",
    "training_method_name_subtokens = np.expand_dims(training_data_tensors['name_tokens'], axis=-1)\n",
    "\n",
    "validating_dataset = (np.expand_dims(validating_data_tensors['body_tokens'], axis=-1),\n",
    "                      np.expand_dims(validating_data_tensors['name_tokens'], axis=-1))\n",
    "\n",
    "testing_dataset = (np.expand_dims(testing_data_tensors['body_tokens'], axis=-1),\n",
    "                   np.expand_dims(testing_data_tensors['name_tokens'], axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAttention: mask_vector shape = (1, ?, 1)\nConvAttention: Tokens shape = (1, ?, 1, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAttention: h_t shape = (1, ?, 8)\nAttentionFeatures: C shape = (1, ?, 1, 128), h_t shape = (1, ?, 8)\nAttentionFeatures: L_1 shape = (1, ?, 1, 8)\nAttentionFeatures: L_2 shape = (1, ?, 1, 8)\nAttentionFeatures: L_2 shape  after multiply = (1, ?, ?, 8)\nAttentionFeatures: L_feat shape = (1, ?, ?, 8)\nConvAttention: L_feat shape = (1, ?, ?, 8)\nAttentionWeights: l_feat shape = (1, ?, ?, 8)\nAttentionWeights: attention_weight shape = (1, ?, ?, 1)\nConvAttention: alpha shape = (1, ?, ?)\nConvAttention: n_hat shape = (1, ?, 128)\nConvAttention: E shape = (842, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAttention: n_hat_E shape = (1, ?, 842)\nConvAttention: n shape = (1, ?, 842)\nTrain on 702 samples, validate on 89 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/samialab/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00001: val_acc improved from -inf to 0.90045, saving model to trained_models/cnn-attention-no-unit-tests/elasticsearch/01/weights-01-0.90.hdf5\n - 15s - loss: 1.6640 - acc: 0.8289 - val_loss: 0.6849 - val_acc: 0.9004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00002: val_acc did not improve from 0.90045\n - 14s - loss: 0.6469 - acc: 0.8974 - val_loss: 0.6078 - val_acc: 0.9004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00003: val_acc did not improve from 0.90045\n - 14s - loss: 0.5775 - acc: 0.8980 - val_loss: 0.6229 - val_acc: 0.8989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0d84412511dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidating_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     )\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# tensorboard = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from models.cnn_attention import ConvAttention\n",
    "\n",
    "# Optimised hyperparameter are reported in page 5 of the paper\n",
    "\n",
    "batch_size = hyperparameter['batch_size']\n",
    "main_input = layers.Input(shape=(None, 1),\n",
    "                          batch_size=batch_size,\n",
    "                          dtype=tf.int32, name='main_input',\n",
    "                          )\n",
    "cnn_layer = ConvAttention(hyperparameter)\n",
    "optimizer = keras.optimizers.Nadam()  # RMSprop with Nesterov momentum\n",
    "loss_func = keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "# define execution\n",
    "cnn_output = cnn_layer(main_input)\n",
    "\n",
    "model = keras.Model(inputs=[main_input], outputs=cnn_output)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_func,\n",
    "              metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "# checkpoint\n",
    "directory = \"trained_models/cnn-attention-no-unit-tests/elasticsearch/01\"\n",
    "filepath = \"{}/weights-{{epoch:02d}}-{{val_acc:.2f}}.hdf5\".format(directory)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True,\n",
    "                             mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "with open('{}/model_config.json'.format(directory), 'w') as fp:\n",
    "    json.dump(hyperparameter, fp)\n",
    "with open('{}/preprocessing_config.json'.format(directory), 'w') as fp:\n",
    "    json.dump(training_dataset_preprocessor.config, fp)\n",
    "\n",
    "history = model.fit(training_body_subtokens,\n",
    "                    training_method_name_subtokens,\n",
    "                    epochs=30,\n",
    "                    verbose=2,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validating_dataset,\n",
    "                    )\n",
    "# tensorboard = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True,\n",
    "#                                          batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8ff756a82782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.03625512123108\n"
     ]
    }
   ],
   "source": [
    "# # overfit and evaluate the model \n",
    "loss, accuracy = model.evaluate(testing_dataset[0], testing_dataset[1], batch_size=hyperparameter['batch_size'], verbose=0)\n",
    "print('Accuracy: {}'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 35 20 87 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]]\n"
     ]
    }
   ],
   "source": [
    "# translate prediction\n",
    "\n",
    "from data.utils import translate_tokenized_array_to_list_words\n",
    "\n",
    "# prediction = model.predict(np.expand_dims(code_snippet[8], 0), steps=1)\n",
    "prediction = model.predict(training_body_subtokens[9:10], steps=1)\n",
    "print(prediction.argmax(2))\n",
    "# translate_tokenized_array_to_list_words(vocab, [10])\n",
    "# predict_name(vocab, model, code_snippet[21].reshape(1, -1))\n",
    "# translate_tokenized_array_to_list_words(vocab, prediction.argmax(2)[0])\n",
    "# print(prediction2.argmax(2))\n",
    "# prediction.argmax(2)\n",
    "# label_name.shape\n",
    "# code_snippet.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'resolve', 'validate', 'and', '</s>']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.utils import translate_tokenized_array_to_list_words\n",
    "\n",
    "translate_tokenized_array_to_list_words(vocab, np.asarray([ 10, 473, 461, 148,  11]))\n",
    "# translate_tokenized_array_to_list_words(vocab, label_name[10].reshape(1, -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(np.expand_dims(training_body_subtokens[10], axis=0), steps=1, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 10, 473, 148,   0]]),\n array([[ 10, 473, 148,  11,   0]]),\n array([[ 10, 473, 461, 148,   0]]),\n array([[ 10, 473, 461, 148,  11,   0]]),\n array([[ 10, 473,  39, 148,   0]]),\n array([[ 10, 473, 148, 461,   0]]),\n array([[ 10, 473, 461,  11,   0]]),\n array([[ 10, 473, 461,   0]]),\n array([[ 10, 473,  39, 148,  11,   0]]),\n array([[ 10, 473, 148, 461,  11,   0]])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val, probs = tf.keras.backend.ctc_decode(\n",
    "    prediction,\n",
    "    (max_chunk_length, )*1,\n",
    "    greedy=False,\n",
    "    beam_width=100,\n",
    "    top_paths=5\n",
    ")\n",
    "\n",
    "tf.Session().run(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-e278479257fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbeamsearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbeamsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_id_or_unk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCE_START_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_id_or_unk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCE_END_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/r252/method-name-prediction/src/utils/activations.py\u001b[0m in \u001b[0;36mbeamsearch\u001b[0;34m(probabilities_function, start_token_id, end_token_id, beam_width, clip_len)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Get probability of each possible next word for the incomplete prefix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobabilities_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnext_word\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mend_token_id\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if next word is the end token then mark prefix as complete and leave out the end token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0mcurr_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from data.constants import SENTENCE_START_TOKEN, SENTENCE_END_TOKEN\n",
    "from utils.activations import beamsearch\n",
    "\n",
    "beamsearch(prediction, vocab.get_id_or_unk(SENTENCE_START_TOKEN), vocab.get_id_or_unk(SENTENCE_END_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.3551938e-10, 2.8790705e-16, 1.0384232e-24, ...,\n         7.7062481e-27, 6.6031810e-27, 6.5961568e-27],\n        [5.1365804e-04, 7.7208642e-06, 1.0618540e-14, ...,\n         5.6013923e-13, 5.9329511e-13, 5.6318811e-13],\n        [5.4057338e-04, 1.2760347e-05, 1.3695989e-13, ...,\n         3.2022605e-12, 3.5857461e-12, 3.3639059e-12],\n        [3.6370161e-04, 7.1513418e-06, 6.8486043e-14, ...,\n         1.1194668e-12, 1.2599305e-12, 1.1756372e-12],\n        [2.1558027e-03, 1.1601456e-05, 2.2902806e-13, ...,\n         1.4900788e-12, 1.6213416e-12, 1.5328699e-12]]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(np.expand_dims(np.expand_dims(np.asarray([10, 473, 461, 148,  11]), 0), -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
