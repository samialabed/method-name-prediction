{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 27 2019 \n\nCPython 3.6.8\nIPython 7.2.0\n\nnumpy 1.15.4\nscipy 1.2.0\nsklearn not installed\npandas 0.23.4\ntensorflow 1.12.0\nkeras 2.2.4\n\ncompiler   : GCC 7.3.0\nsystem     : Linux\nrelease    : 4.15.0-43-generic\nmachine    : x86_64\nprocessor  : x86_64\nCPU cores  : 8\ninterpreter: 64bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext watermark\n",
    "%autoreload 2\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas,tensorflow,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO visualise and summarise the corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.preprocess import PreProcessor\n",
    "\n",
    "data = PreProcessor(config=PreProcessor.DEFAULT_CONFIG,\n",
    "                    data_dir='data/raw/r252-corpus-features/org/elasticsearch/action/admin/cluster/allocation/')\n",
    "\n",
    "vocab = data.metadata['token_vocab']\n",
    "processed = data.get_tensorise_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 319 Code snippet len: 135 label_name len: 135\nConvAttention: Tokens shape = (1, 50, 128), h_t shape = (1, 8)\nAttentionFeatures: C shape = (1, 50, 128), h_t shape = (1, 8)\nAttentionFeatures: L_1 shape = (1, 50, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionFeatures: L_2 shape = (1, 50, 8)\nAttentionFeatures: L_2 shape  after multiply = (1, 50, 8)\nAttentionFeatures: L_feat shape = Tensor(\"conv_attention_1/attention_features_1/l2_normalize:0\", shape=(1, 50, 8), dtype=float32)\nConvAttention: L_feat shape = (1, 50, 8)\nAttentionWeights: l_feat shape = (1, 50, 8)\nAttentionWeights: attention_weight shape = (1, 50, 1)\nConvAttention: n_hat shape = (1, 128)\nConvAttention: n shape = (128, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samialab/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 1607.3305 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1607.3305 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3436s - loss: 1607.3305 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1607.3305 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1607.3305 - acc: 0.0013\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "import numpy as np\n",
    "from models.cnn_attention import ConvAttention\n",
    "\n",
    "embedding_dim = 128\n",
    "vocabulary_size = len(vocab)\n",
    "max_chunk_length = data.config['max_chunk_length']\n",
    "code_snippet = processed['body_tokens']\n",
    "label_name = np.expand_dims(processed['name_tokens'], 2)\n",
    "# label_name = keras.utils.to_categorical(processed['name_tokens'], num_classes=vocabulary_size)\n",
    "print(\"Vocab Size: {} Code snippet len: {} label_name len: {}\".format(vocabulary_size, len(code_snippet), len(label_name)))\n",
    "\n",
    "# TODO make the input a json file and parse it\n",
    "batch_size = 1\n",
    "k1 = 8\n",
    "k2 = 8\n",
    "w1 = 24\n",
    "w2 = 29\n",
    "w3 = 10\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Optimised hyperparameter are reported in page 5 of the paper\n",
    "\n",
    "# define layers\n",
    "main_input = layers.Input(shape=(max_chunk_length,),\n",
    "                          batch_size=batch_size,\n",
    "                          dtype='int32', name='main_input')\n",
    "\n",
    "cnn_layer = ConvAttention(vocabulary_size=vocabulary_size,\n",
    "                          embedding_dim=embedding_dim,\n",
    "                          max_chunk_length=max_chunk_length,\n",
    "                          k1=k1,\n",
    "                          k2=k2,\n",
    "                          w1=w1,\n",
    "                          w2=w2,\n",
    "                          w3=w3,\n",
    "                          dropout_rate=dropout_rate)\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()  # RMSprop with Nesterov momentum\n",
    "loss_func = keras.losses.mean_squared_error\n",
    "\n",
    "# define execution\n",
    "cnn_output = cnn_layer(main_input)\n",
    "model = keras.Model(inputs=[main_input], outputs=cnn_output)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_func,\n",
    "              metrics=['accuracy'])\n",
    "# fit the model\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = model.fit(code_snippet, label_name, epochs=5, verbose=2, batch_size=batch_size, callbacks=[tbCallBack])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 50, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name = 'conv_attention'\n",
    "model.predict(code_snippet[0].reshape(1, -1), steps=1).shape\n",
    "\n",
    "# model.predict(code_snippet[0].reshape(1, -1), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmain_input (InputLayer)      (1, 50)                   0         \n_________________________________________________________________\nconv_attention (ConvAttentio (128, 50, 1)              70968     \n=================================================================\nTotal params: 70,968\nTrainable params: 70,968\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1333333303531011\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# # overfit and evaluate the model \n",
    "loss, accuracy = model.evaluate(code_snippet, label_name, batch_size=1, verbose=0)\n",
    "print('Accuracy: {}'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 50, 319)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translate prediction\n",
    "\n",
    "from data.utils import translate_tokenized_array_to_list_words\n",
    "\n",
    "# prediction = model.predict(code_snippet[5].reshape(1, -1))\n",
    "# translate_tokenized_array_to_list_words(vocab, processed['body_tokens'][1])\n",
    "# print(prediction)\n",
    "# vocab.get_name_for_id(317)\n",
    "# processed['body_tokens'][0]\n",
    "\n",
    "keras.utils.to_categorical(processed['name_tokens'], num_classes=vocabulary_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_snippet[0].reshape(1, -1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
