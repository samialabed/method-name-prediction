{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 24 2019 \n\nCPython 3.6.8\nIPython 7.2.0\n\nnumpy 1.15.4\nscipy 1.2.0\nsklearn not installed\npandas 0.23.4\ntensorflow 1.12.0\nkeras 2.2.4\n\ncompiler   : GCC 7.3.0\nsystem     : Linux\nrelease    : 4.15.0-43-generic\nmachine    : x86_64\nprocessor  : x86_64\nCPU cores  : 8\ninterpreter: 64bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext watermark\n",
    "%autoreload 2\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas,tensorflow,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO visualise and summarise the corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.preprocess import PreProcessor\n",
    "\n",
    "data = PreProcessor(config=PreProcessor.DEFAULT_CONFIG,\n",
    "                    data_dir='data/raw/r252-corpus-features/org/elasticsearch/action/admin/cluster/allocation/')\n",
    "\n",
    "vocab = data.metadata['token_vocab']\n",
    "processed = data.get_tensorise_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = np.expand_dims(processed['name_tokens'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAttention: Tokens shape = (1, 50, 128), h_t shape = (1, 8)\nAttentionFeatures: C shape = (1, 50, 128), h_t shape = (1, 8)\nAttentionFeatures: L_1 shape = (1, 50, 8)\nAttentionFeatures: L_2 shape = (1, 50, 8)\nAttentionFeatures: L_2 shape  after multiply = (1, 50, 8)\nAttentionFeatures: L_feat shape = Tensor(\"conv_attention_8/attention_features_8/l2_normalize:0\", shape=(1, 50, 8), dtype=float32)\nConvAttention: L_feat shape = (1, 50, 8)\nAttentionWeights: l_feat shape = (1, 50, 8)\nAttentionWeights: attention_weight shape = (1, 50, 1)\nConvAttention: n_hat shape = (1, 128)\nConvAttention: softmax shape = (128, 50, 1)\nCode snippet len: 135 label_name len: 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samialab/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 1603.9984 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1603.9984 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1603.9984 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1603.9984 - acc: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1603.9984 - acc: 0.0013\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "\n",
    "from models.cnn_attention import ConvAttention\n",
    "\n",
    "code_snippet = processed['body_tokens']\n",
    "label_name = np.expand_dims(processed['name_tokens'], 2)\n",
    "\n",
    "embedding_dim = 128\n",
    "vocabulary_size = len(vocab)\n",
    "max_chunk_length = data.config['max_chunk_length']\n",
    "\n",
    "# TODO make the input a json file and parse it\n",
    "batch_size = 1\n",
    "k1 = 8\n",
    "k2 = 8\n",
    "w1 = 24\n",
    "w2 = 29\n",
    "w3 = 10\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Optimised hyperparameter are reported in page 5 of the paper\n",
    "\n",
    "# define layers\n",
    "main_input = layers.Input(shape=(max_chunk_length,),\n",
    "                          batch_size=1,\n",
    "                          dtype='int32', name='main_input')\n",
    "\n",
    "cnn_layer = ConvAttention(vocabulary_size=vocabulary_size,\n",
    "                          embedding_dim=embedding_dim,\n",
    "                          max_chunk_length=max_chunk_length,\n",
    "                          k1=k1,\n",
    "                          k2=k2,\n",
    "                          w1=w1,\n",
    "                          w2=w2,\n",
    "                          w3=w3,\n",
    "                          dropout_rate=dropout_rate)\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()  # RMSprop with Nesterov momentum\n",
    "loss_func = keras.losses.mean_squared_error\n",
    "\n",
    "# define execution\n",
    "cnn_output = cnn_layer(main_input)\n",
    "model = keras.Model(inputs=[main_input], outputs=cnn_output)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_func,\n",
    "              metrics=['accuracy'])\n",
    "# fit the model\n",
    "print(\"Code snippet len: {} label_name len: {}\".format(len(code_snippet), len(label_name)))\n",
    "history = model.fit(code_snippet, label_name, epochs=5, verbose=2, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nmain_input (InputLayer)         (1, 50)              0                                            \n__________________________________________________________________________________________________\nembedding_33 (Embedding)        (1, 50, 128)         40832       main_input[0][0]                 \n__________________________________________________________________________________________________\ngru_33 (GRU)                    [(1, 8), (1, 8)]     3288        embedding_33[0][0]               \n__________________________________________________________________________________________________\nconv_attention_33 (ConvAttentio (1, 50, 1)           26529       embedding_33[0][0]               \n                                                                 gru_33[0][1]                     \n==================================================================================================\nTotal params: 70,649\nTrainable params: 70,649\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# # overfit and evaluate the model \n",
    "# loss, accuracy = model.evaluate(code_snippet, label_name, verbose=0)\n",
    "# print('Accuracy: {}'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translate prediction\n",
    "\n",
    "from data.utils import translate_tokenized_array_to_list_words\n",
    "\n",
    "# prediction = model.predict(code_snippet[5].reshape(1, -1))\n",
    "# translate_tokenized_array_to_list_words(vocab, [0])\n",
    "# print(prediction)\n",
    "# vocab.get_name_for_id(317)\n",
    "code_snippet[0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
