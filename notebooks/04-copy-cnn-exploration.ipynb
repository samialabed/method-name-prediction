{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 10 2019 \n\nCPython 3.6.8\nIPython 7.2.0\n\nnumpy 1.15.4\nscipy 1.2.0\nsklearn 0.20.3\npandas 0.23.4\ntensorflow 1.13.1\nkeras 2.2.4\n\ncompiler   : GCC 7.3.0\nsystem     : Linux\nrelease    : 4.15.0-45-generic\nmachine    : x86_64\nprocessor  : x86_64\nCPU cores  : 8\ninterpreter: 64bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext watermark\n",
    "\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas,tensorflow,keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"run_name\": \"copy-cnv-test\",\n",
    "    \"model_type\": \"copy_attention\",\n",
    "    \"model_hyperparameters\": {\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 1,\n",
    "        \"k1\": 32,\n",
    "        \"k2\": 16,\n",
    "        \"w1\": 18,\n",
    "        \"w2\": 19,\n",
    "        \"w3\": 2,\n",
    "        \"dropout_rate\": 0,  # TODO make it 0.4\n",
    "        \"max_chunk_length\": 50,\n",
    "        \"embedding_dim\": 128,\n",
    "    },\n",
    "    \"beam_search_config\": {\n",
    "        \"beam_width\": 5,\n",
    "        \"beam_top_paths\": 5\n",
    "    },\n",
    "    \"preprocessor_config\": {\n",
    "        \"vocabulary_max_size\": 5000,\n",
    "        \"max_chunk_length\": 50,\n",
    "        \"vocabulary_count_threshold\": 3,\n",
    "        \"min_line_of_codes\": 3,\n",
    "        \"skip_tests\": True\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # files: 377\nTraining Data: 236, Testing Data: 114, Validating data: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samialab/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data.preprocess import PreProcessor, get_data_files_from_directory\n",
    "\n",
    "# Move this to a config file\n",
    "all_files = get_data_files_from_directory(data_dir='data/raw/r252-corpus-features/org/elasticsearch/action/admin',\n",
    "                                          skip_tests=hyperparameters['preprocessor_config']['skip_tests'])\n",
    "print(\"Total # files: {}\".format(len(all_files)))\n",
    "train_data_files, test_data_files = train_test_split(all_files, train_size=0.7)\n",
    "train_data_files, validate_data_files = train_test_split(train_data_files, train_size=0.9)\n",
    "print(\"Training Data: {}, Testing Data: {}, Validating data: {}\".format(len(train_data_files),\n",
    "                                                                        len(test_data_files),\n",
    "                                                                        len(validate_data_files)))\n",
    "training_dataset_preprocessor = PreProcessor(config=hyperparameters['preprocessor_config'],\n",
    "                                             data_files=train_data_files)\n",
    "validating_dataset_preprocessor = PreProcessor(config=hyperparameters['preprocessor_config'],\n",
    "                                               data_files=validate_data_files,\n",
    "                                               vocabulary=training_dataset_preprocessor.vocabulary)\n",
    "testing_dataset_preprocessor = PreProcessor(config=hyperparameters['preprocessor_config'],\n",
    "                                            data_files=test_data_files,\n",
    "                                            vocabulary=training_dataset_preprocessor.vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = training_dataset_preprocessor.vocabulary\n",
    "vocabulary_size = len(vocab) + 1\n",
    "max_chunk_length = training_dataset_preprocessor.config['max_chunk_length']\n",
    "training_data_tensors = training_dataset_preprocessor.get_tensorise_data()\n",
    "testing_data_tensors = testing_dataset_preprocessor.get_tensorise_data()\n",
    "validating_data_tensors = validating_dataset_preprocessor.get_tensorise_data()\n",
    "\n",
    "# code_snippet = processed['body_tokens']\n",
    "training_body_subtokens = np.expand_dims(training_data_tensors['body_tokens'], axis=-1)\n",
    "training_method_name_subtokens = np.expand_dims(training_data_tensors['name_tokens'], axis=-1)\n",
    "\n",
    "validating_dataset = (np.expand_dims(validating_data_tensors['body_tokens'], axis=-1),\n",
    "                      np.expand_dims(validating_data_tensors['name_tokens'], axis=-1))\n",
    "\n",
    "testing_dataset = (np.expand_dims(testing_data_tensors['body_tokens'], axis=-1),\n",
    "                   np.expand_dims(testing_data_tensors['name_tokens'], axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:mask_vector shape = (1, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:Tokens shape = (1, 50, 1, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:h_t shape = (1, 50, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:C shape = (1, 50, 1, 128), h_t shape = (1, 50, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:L_1 shape = (1, 50, 1, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:L_2 shape = (1, 50, 1, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:L_2 shape  after multiply = (1, 50, 50, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:L_feat shape = (1, 50, 50, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:L_feat shape = (1, 50, 50, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:L_feat shape = (1, 50, 50, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:attention_weight shape = (1, 50, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:alpha shape = (1, 50, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:n_hat shape = (1, 50, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:E shape = (468, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:n_hat_E shape = (1, 50, 468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:n shape = (1, 50, 468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:Copy_CNN_attention: n shape: (1, 50, 468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:L_feat shape = (1, 50, 50, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.attention:attention_weight shape = (1, 50, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:kappa shape: (1, 50, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:lmda shape: (1, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:pos2voc shape: (1, 50, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:weighted_n shape:(1, 50, 468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.copy_cnn_attention:weighted_pos2voc shape:(1, 50, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model objective: input_code_subtoken.shape: (1, 50, 1)\nModel objective: copy_probability.shape: (1, 50, 1)\nModel objective: copy_weights.shape: (1, 50, 128)\nModel objective: y_pred.shape: (1, 50, 468)\nModel objective: I_C.shape: (?, 50, 1)\nModel objective: probability_correct_copy.shape: (1, 50, 1)\nModel objective: probability_target_token.shape: (?, 50, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 26s - loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-80b17286d397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0;31m# validation_data=validating_dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/method-name-prediction/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "\n",
    "from models.copy_cnn_attention import CopyAttention, model_objective\n",
    "\n",
    "I_C = np.array([np.isin(x, y) for (x, y) in zip(training_body_subtokens, training_method_name_subtokens)])\n",
    "\n",
    "model_hyperparameters = hyperparameters['model_hyperparameters']\n",
    "model_hyperparameters[\"vocabulary_size\"] = vocabulary_size\n",
    "batch_size = model_hyperparameters['batch_size']\n",
    "main_input = layers.Input(shape=(max_chunk_length, 1), batch_size=batch_size, dtype=tf.int32, name='main_input')\n",
    "\n",
    "copy_cnn_layer = CopyAttention(model_hyperparameters)\n",
    "optimizer = keras.optimizers.Nadam()  # RMSprop with Nesterov momentum\n",
    "\n",
    "# define execution\n",
    "copy_weights, n_to_map, copy_probability = copy_cnn_layer(main_input)\n",
    "\n",
    "loss_func = model_objective(main_input, copy_probability, copy_weights)\n",
    "\n",
    "model = keras.Model(inputs=[main_input], outputs=n_to_map)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_func,\n",
    "              # metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "history = model.fit(training_body_subtokens,\n",
    "                    training_method_name_subtokens.astype('int32'),\n",
    "                    epochs=model_hyperparameters['epochs'],\n",
    "                    verbose=2,\n",
    "                    batch_size=batch_size,\n",
    "                    # validation_data=validating_dataset,\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
